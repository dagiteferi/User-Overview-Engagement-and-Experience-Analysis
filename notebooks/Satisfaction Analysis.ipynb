{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am tasked with working for an investor who specializes in purchasing undervalued assets. The investor wants to perform detailed data analysis to identify opportunities for growth and profitability in a potential purchaseâ€”TellCo, a mobile service provider in the Republic of Pefkakia. my goal is to analyze customer data, provide insights, and make recommendations on whether TellCo is worth buying or selling. The analysis will be presented through a web-based dashboard and a written report.\n",
    "Task 2: User Engagement Analysis\n",
    "Objective: Assess user engagement using metrics such as session frequency, duration, and total traffic.\n",
    "\n",
    "Task 4: Satisfaction Analysis\n",
    "Objective: Analyze customer satisfaction based on engagement and experience.\n",
    "\n",
    "Assign engagement and experience scores to each user using Euclidean distance.\n",
    "\n",
    "Calculate satisfaction scores and report top satisfied customers.\n",
    "\n",
    "Build a regression model to predict satisfaction scores.\n",
    "\n",
    "Perform k-means clustering on engagement and experience scores.\n",
    "\n",
    "Aggregate and report satisfaction and experience scores per cluster.\n",
    "\n",
    "Export final user data with scores to a local MySQL database and report a screenshot of the query output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "os.chdir('..')\n",
    "sys.path.append(os.getcwd())\n",
    "from src.Eda import handle_missing_values, remove_outliers_iqr\n",
    "#from src.Eda import missing_values_table, convert_bytes_to_megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL database!\n",
      "      Bearer Id            Start  Start ms              End  End ms  \\\n",
      "0  1.311448e+19   4/4/2019 12:01     770.0  4/25/2019 14:35   662.0   \n",
      "1  1.311448e+19   4/9/2019 13:04     235.0   4/25/2019 8:15   606.0   \n",
      "2  1.311448e+19   4/9/2019 17:42       1.0  4/25/2019 11:58   652.0   \n",
      "3  1.311448e+19   4/10/2019 0:31     486.0   4/25/2019 7:36   171.0   \n",
      "4  1.311448e+19  4/12/2019 20:10     565.0  4/25/2019 10:40   954.0   \n",
      "\n",
      "   Dur. (ms)          IMSI  MSISDN/Number          IMEI  \\\n",
      "0  1823652.0  2.082014e+14   3.366496e+10  3.552121e+13   \n",
      "1  1365104.0  2.082019e+14   3.368185e+10  3.579401e+13   \n",
      "2  1361762.0  2.082003e+14   3.376063e+10  3.528151e+13   \n",
      "3  1321509.0  2.082014e+14   3.375034e+10  3.535661e+13   \n",
      "4  1089009.0  2.082014e+14   3.369980e+10  3.540701e+13   \n",
      "\n",
      "      Last Location Name  ...  Youtube DL (Bytes)  Youtube UL (Bytes)  \\\n",
      "0  9.16456699548519E+015  ...          15854611.0           2501332.0   \n",
      "1                L77566A  ...          20247395.0          19111729.0   \n",
      "2                D42335A  ...          19725661.0          14699576.0   \n",
      "3                T21824A  ...          21388122.0          15146643.0   \n",
      "4                D88865A  ...          15259380.0          18962873.0   \n",
      "\n",
      "   Netflix DL (Bytes)  Netflix UL (Bytes)  Gaming DL (Bytes)  \\\n",
      "0           8198936.0           9656251.0        278082303.0   \n",
      "1          18338413.0          17227132.0        608750074.0   \n",
      "2          17587794.0           6163408.0        229584621.0   \n",
      "3          13994646.0           1097942.0        799538153.0   \n",
      "4          17124581.0            415218.0        527707248.0   \n",
      "\n",
      "   Gaming UL (Bytes)  Other DL (Bytes)  Other UL (Bytes)  Total UL (Bytes)  \\\n",
      "0         14344150.0       171744450.0         8814393.0        36749741.0   \n",
      "1          1170709.0       526904238.0        15055145.0        53800391.0   \n",
      "2           395630.0       410692588.0         4215763.0        27883638.0   \n",
      "3         10849722.0       749039933.0        12797283.0        43324218.0   \n",
      "4          3529801.0       550709500.0        13910322.0        38542814.0   \n",
      "\n",
      "   Total DL (Bytes)  \n",
      "0       308879636.0  \n",
      "1       653384965.0  \n",
      "2       279807335.0  \n",
      "3       846028530.0  \n",
      "4       569138589.0  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "Connection closed.\n",
      "Connected to PostgreSQL database!\n",
      "      Bearer Id            Start  Start ms              End  End ms  \\\n",
      "0  1.311448e+19   4/4/2019 12:01     770.0  4/25/2019 14:35   662.0   \n",
      "1  1.311448e+19   4/9/2019 13:04     235.0   4/25/2019 8:15   606.0   \n",
      "2  1.311448e+19   4/9/2019 17:42       1.0  4/25/2019 11:58   652.0   \n",
      "3  1.311448e+19   4/10/2019 0:31     486.0   4/25/2019 7:36   171.0   \n",
      "4  1.311448e+19  4/12/2019 20:10     565.0  4/25/2019 10:40   954.0   \n",
      "\n",
      "   Dur. (ms)          IMSI  MSISDN/Number          IMEI  \\\n",
      "0  1823652.0  2.082014e+14   3.366496e+10  3.552121e+13   \n",
      "1  1365104.0  2.082019e+14   3.368185e+10  3.579401e+13   \n",
      "2  1361762.0  2.082003e+14   3.376063e+10  3.528151e+13   \n",
      "3  1321509.0  2.082014e+14   3.375034e+10  3.535661e+13   \n",
      "4  1089009.0  2.082014e+14   3.369980e+10  3.540701e+13   \n",
      "\n",
      "      Last Location Name  ...  Youtube DL (Bytes)  Youtube UL (Bytes)  \\\n",
      "0  9.16456699548519E+015  ...          15854611.0           2501332.0   \n",
      "1                L77566A  ...          20247395.0          19111729.0   \n",
      "2                D42335A  ...          19725661.0          14699576.0   \n",
      "3                T21824A  ...          21388122.0          15146643.0   \n",
      "4                D88865A  ...          15259380.0          18962873.0   \n",
      "\n",
      "   Netflix DL (Bytes)  Netflix UL (Bytes)  Gaming DL (Bytes)  \\\n",
      "0           8198936.0           9656251.0        278082303.0   \n",
      "1          18338413.0          17227132.0        608750074.0   \n",
      "2          17587794.0           6163408.0        229584621.0   \n",
      "3          13994646.0           1097942.0        799538153.0   \n",
      "4          17124581.0            415218.0        527707248.0   \n",
      "\n",
      "   Gaming UL (Bytes)  Other DL (Bytes)  Other UL (Bytes)  Total UL (Bytes)  \\\n",
      "0         14344150.0       171744450.0         8814393.0        36749741.0   \n",
      "1          1170709.0       526904238.0        15055145.0        53800391.0   \n",
      "2           395630.0       410692588.0         4215763.0        27883638.0   \n",
      "3         10849722.0       749039933.0        12797283.0        43324218.0   \n",
      "4          3529801.0       550709500.0        13910322.0        38542814.0   \n",
      "\n",
      "   Total DL (Bytes)  \n",
      "0       308879636.0  \n",
      "1       653384965.0  \n",
      "2       279807335.0  \n",
      "3       846028530.0  \n",
      "4       569138589.0  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "from scripts.DB_connection import PostgresConnection\n",
    "\n",
    "# Establishing the database connection\n",
    "db = PostgresConnection()\n",
    "db.connect()\n",
    "\n",
    "if db.conn:\n",
    "    # Example query\n",
    "    query = \"SELECT * FROM xdr_data\"\n",
    "    result = db.execute_query(query)\n",
    "\n",
    "    if result:\n",
    "        # Convert the result to a Pandas DataFrame\n",
    "        df = pd.DataFrame(result, columns=[desc[0] for desc in db.cursor.description])\n",
    "        print(df.head())  # Display the first few rows of the DataFrame\n",
    "    else:\n",
    "        print(\"No results returned from the query.\")\n",
    "    \n",
    "    # Close the connection when done\n",
    "    db.close_connection()\n",
    "else:\n",
    "    print(\"Error: No database connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150001 entries, 0 to 150000\n",
      "Data columns (total 55 columns):\n",
      " #   Column                                    Non-Null Count   Dtype  \n",
      "---  ------                                    --------------   -----  \n",
      " 0   Bearer Id                                 149010 non-null  float64\n",
      " 1   Start                                     150000 non-null  object \n",
      " 2   Start ms                                  150000 non-null  float64\n",
      " 3   End                                       150000 non-null  object \n",
      " 4   End ms                                    150000 non-null  float64\n",
      " 5   Dur. (ms)                                 150000 non-null  float64\n",
      " 6   IMSI                                      149431 non-null  float64\n",
      " 7   MSISDN/Number                             148935 non-null  float64\n",
      " 8   IMEI                                      149429 non-null  float64\n",
      " 9   Last Location Name                        148848 non-null  object \n",
      " 10  Avg RTT DL (ms)                           122172 non-null  float64\n",
      " 11  Avg RTT UL (ms)                           122189 non-null  float64\n",
      " 12  Avg Bearer TP DL (kbps)                   150000 non-null  float64\n",
      " 13  Avg Bearer TP UL (kbps)                   150000 non-null  float64\n",
      " 14  TCP DL Retrans. Vol (Bytes)               61855 non-null   float64\n",
      " 15  TCP UL Retrans. Vol (Bytes)               53352 non-null   float64\n",
      " 16  DL TP < 50 Kbps (%)                       149247 non-null  float64\n",
      " 17  50 Kbps < DL TP < 250 Kbps (%)            149247 non-null  float64\n",
      " 18  250 Kbps < DL TP < 1 Mbps (%)             149247 non-null  float64\n",
      " 19  DL TP > 1 Mbps (%)                        149247 non-null  float64\n",
      " 20  UL TP < 10 Kbps (%)                       149209 non-null  float64\n",
      " 21  10 Kbps < UL TP < 50 Kbps (%)             149209 non-null  float64\n",
      " 22  50 Kbps < UL TP < 300 Kbps (%)            149209 non-null  float64\n",
      " 23  UL TP > 300 Kbps (%)                      149209 non-null  float64\n",
      " 24  HTTP DL (Bytes)                           68527 non-null   float64\n",
      " 25  HTTP UL (Bytes)                           68191 non-null   float64\n",
      " 26  Activity Duration DL (ms)                 150000 non-null  float64\n",
      " 27  Activity Duration UL (ms)                 150000 non-null  float64\n",
      " 28  Dur. (ms).1                               150000 non-null  float64\n",
      " 29  Handset Manufacturer                      149429 non-null  object \n",
      " 30  Handset Type                              149429 non-null  object \n",
      " 31  Nb of sec with 125000B < Vol DL           52463 non-null   float64\n",
      " 32  Nb of sec with 1250B < Vol UL < 6250B     57107 non-null   float64\n",
      " 33  Nb of sec with 31250B < Vol DL < 125000B  56415 non-null   float64\n",
      " 34  Nb of sec with 37500B < Vol UL            19747 non-null   float64\n",
      " 35  Nb of sec with 6250B < Vol DL < 31250B    61684 non-null   float64\n",
      " 36  Nb of sec with 6250B < Vol UL < 37500B    38158 non-null   float64\n",
      " 37  Nb of sec with Vol DL < 6250B             149246 non-null  float64\n",
      " 38  Nb of sec with Vol UL < 1250B             149208 non-null  float64\n",
      " 39  Social Media DL (Bytes)                   150001 non-null  float64\n",
      " 40  Social Media UL (Bytes)                   150001 non-null  float64\n",
      " 41  Google DL (Bytes)                         150001 non-null  float64\n",
      " 42  Google UL (Bytes)                         150001 non-null  float64\n",
      " 43  Email DL (Bytes)                          150001 non-null  float64\n",
      " 44  Email UL (Bytes)                          150001 non-null  float64\n",
      " 45  Youtube DL (Bytes)                        150001 non-null  float64\n",
      " 46  Youtube UL (Bytes)                        150001 non-null  float64\n",
      " 47  Netflix DL (Bytes)                        150001 non-null  float64\n",
      " 48  Netflix UL (Bytes)                        150001 non-null  float64\n",
      " 49  Gaming DL (Bytes)                         150001 non-null  float64\n",
      " 50  Gaming UL (Bytes)                         150001 non-null  float64\n",
      " 51  Other DL (Bytes)                          150001 non-null  float64\n",
      " 52  Other UL (Bytes)                          150001 non-null  float64\n",
      " 53  Total UL (Bytes)                          150000 non-null  float64\n",
      " 54  Total DL (Bytes)                          150000 non-null  float64\n",
      "dtypes: float64(50), object(5)\n",
      "memory usage: 62.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lest use deferent approch instade ofusing sime cols instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after outlier removal: 150001\n",
      "\n",
      "Total number of outliers removed: 451082\n"
     ]
    }
   ],
   "source": [
    "def handle_outliers_iqr(df):\n",
    "    df_cleaned = df.copy()  # Create a copy of the DataFrame for modifications\n",
    "    outlier_info = {}  # Dictionary to store information about outliers\n",
    "    \n",
    "    for column in df_cleaned.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "        Q1 = df_cleaned[column].quantile(0.25)\n",
    "        Q3 = df_cleaned[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define outlier bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers = df_cleaned[(df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound)]\n",
    "        \n",
    "        # Store the count of outliers\n",
    "        outlier_info[column] = outliers.shape[0]\n",
    "        \n",
    "        # Replace outliers with median of the column\n",
    "        median_value = df_cleaned[column].median()\n",
    "        df_cleaned.loc[(df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound), column] = median_value\n",
    "    \n",
    "    return df_cleaned, outlier_info\n",
    "\n",
    "# Apply the function to handle outliers\n",
    "df_cleaned, outlier_info = handle_outliers_iqr(df)\n",
    "\n",
    "# Display the number of rows after outlier removal\n",
    "print(f\"Number of rows after outlier removal: {df_cleaned.shape[0]}\")\n",
    "\n",
    "# Display information about outliers\n",
    "total_outliers_removed = sum(outlier_info.values())\n",
    "print(f\"\\nTotal number of outliers removed: {total_outliers_removed}\")\n",
    "\n",
    "# Store the cleaned DataFrame in a new DataFrame\n",
    "df_outliers_removed = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after outlier removal: 150001\n",
      "\n",
      "Total number of outliers removed: 451082\n"
     ]
    }
   ],
   "source": [
    "#Looking for outliers and replacing them using advanced methods like imputation.\n",
    "\n",
    "\n",
    "# Function to identify and replace outliers using IQR method\n",
    "def handle_outliers_iqr(df):\n",
    "    df_cleaned = df.copy()  # Create a copy of the DataFrame for modifications\n",
    "    outlier_info = {}  # Dictionary to store information about outliers\n",
    "    \n",
    "    for column in df_cleaned.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "        Q1 = df_cleaned[column].quantile(0.25)\n",
    "        Q3 = df_cleaned[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define outlier bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers = df_cleaned[(df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound)]\n",
    "        \n",
    "        # Store the count of outliers\n",
    "        outlier_info[column] = outliers.shape[0]\n",
    "        \n",
    "        # Replace outliers with median of the column\n",
    "        median_value = df_cleaned[column].median()\n",
    "        df_cleaned.loc[(df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound), column] = median_value\n",
    "    \n",
    "    return df_cleaned, outlier_info\n",
    "\n",
    "# Apply the function to handle outliers\n",
    "df_cleaned, outlier_info = handle_outliers_iqr(df)\n",
    "\n",
    "# Display the number of rows after outlier removal\n",
    "print(f\"Number of rows after outlier removal: {df_cleaned.shape[0]}\")\n",
    "\n",
    "# Display information about outliers\n",
    "total_outliers_removed = sum(outlier_info.values())\n",
    "print(f\"\\nTotal number of outliers removed: {total_outliers_removed}\")\n",
    "\n",
    "# Store the cleaned DataFrame in a new DataFrame\n",
    "df_outliers_removed = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame info after ensuring consistency:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150001 entries, 0 to 150000\n",
      "Data columns (total 55 columns):\n",
      " #   Column                                    Non-Null Count   Dtype  \n",
      "---  ------                                    --------------   -----  \n",
      " 0   Bearer Id                                 150001 non-null  float64\n",
      " 1   Start                                     150001 non-null  object \n",
      " 2   Start ms                                  150001 non-null  float64\n",
      " 3   End                                       150001 non-null  object \n",
      " 4   End ms                                    150001 non-null  float64\n",
      " 5   Dur. (ms)                                 150001 non-null  float64\n",
      " 6   IMSI                                      150001 non-null  float64\n",
      " 7   MSISDN/Number                             150001 non-null  float64\n",
      " 8   IMEI                                      150001 non-null  float64\n",
      " 9   Last Location Name                        150001 non-null  object \n",
      " 10  Avg RTT DL (ms)                           150001 non-null  float64\n",
      " 11  Avg RTT UL (ms)                           150001 non-null  float64\n",
      " 12  Avg Bearer TP DL (kbps)                   150001 non-null  float64\n",
      " 13  Avg Bearer TP UL (kbps)                   150001 non-null  float64\n",
      " 14  TCP DL Retrans. Vol (Bytes)               150001 non-null  float64\n",
      " 15  TCP UL Retrans. Vol (Bytes)               150001 non-null  float64\n",
      " 16  DL TP < 50 Kbps (%)                       150001 non-null  float64\n",
      " 17  50 Kbps < DL TP < 250 Kbps (%)            150001 non-null  float64\n",
      " 18  250 Kbps < DL TP < 1 Mbps (%)             150001 non-null  float64\n",
      " 19  DL TP > 1 Mbps (%)                        150001 non-null  float64\n",
      " 20  UL TP < 10 Kbps (%)                       150001 non-null  float64\n",
      " 21  10 Kbps < UL TP < 50 Kbps (%)             150001 non-null  float64\n",
      " 22  50 Kbps < UL TP < 300 Kbps (%)            150001 non-null  float64\n",
      " 23  UL TP > 300 Kbps (%)                      150001 non-null  float64\n",
      " 24  HTTP DL (Bytes)                           150001 non-null  float64\n",
      " 25  HTTP UL (Bytes)                           150001 non-null  float64\n",
      " 26  Activity Duration DL (ms)                 150001 non-null  float64\n",
      " 27  Activity Duration UL (ms)                 150001 non-null  float64\n",
      " 28  Dur. (ms).1                               150001 non-null  float64\n",
      " 29  Handset Manufacturer                      150001 non-null  object \n",
      " 30  Handset Type                              150001 non-null  object \n",
      " 31  Nb of sec with 125000B < Vol DL           150001 non-null  float64\n",
      " 32  Nb of sec with 1250B < Vol UL < 6250B     150001 non-null  float64\n",
      " 33  Nb of sec with 31250B < Vol DL < 125000B  150001 non-null  float64\n",
      " 34  Nb of sec with 37500B < Vol UL            150001 non-null  float64\n",
      " 35  Nb of sec with 6250B < Vol DL < 31250B    150001 non-null  float64\n",
      " 36  Nb of sec with 6250B < Vol UL < 37500B    150001 non-null  float64\n",
      " 37  Nb of sec with Vol DL < 6250B             150001 non-null  float64\n",
      " 38  Nb of sec with Vol UL < 1250B             150001 non-null  float64\n",
      " 39  Social Media DL (Bytes)                   150001 non-null  float64\n",
      " 40  Social Media UL (Bytes)                   150001 non-null  float64\n",
      " 41  Google DL (Bytes)                         150001 non-null  float64\n",
      " 42  Google UL (Bytes)                         150001 non-null  float64\n",
      " 43  Email DL (Bytes)                          150001 non-null  float64\n",
      " 44  Email UL (Bytes)                          150001 non-null  float64\n",
      " 45  Youtube DL (Bytes)                        150001 non-null  float64\n",
      " 46  Youtube UL (Bytes)                        150001 non-null  float64\n",
      " 47  Netflix DL (Bytes)                        150001 non-null  float64\n",
      " 48  Netflix UL (Bytes)                        150001 non-null  float64\n",
      " 49  Gaming DL (Bytes)                         150001 non-null  float64\n",
      " 50  Gaming UL (Bytes)                         150001 non-null  float64\n",
      " 51  Other DL (Bytes)                          150001 non-null  float64\n",
      " 52  Other UL (Bytes)                          150001 non-null  float64\n",
      " 53  Total UL (Bytes)                          150001 non-null  float64\n",
      " 54  Total DL (Bytes)                          150001 non-null  float64\n",
      "dtypes: float64(50), object(5)\n",
      "memory usage: 62.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Ensure consistency in the dataset\n",
    "def ensure_consistency(df):\n",
    "    # Remove duplicate entries\n",
    "    df_cleaned = df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Convert data types to appropriate types\n",
    "    for column in df_cleaned.columns:\n",
    "        # If the column contains numeric data but is stored as an object, convert it to numeric\n",
    "        if df_cleaned[column].dtype == 'object':\n",
    "            try:\n",
    "                df_cleaned[column] = pd.to_numeric(df_cleaned[column])\n",
    "            except ValueError:\n",
    "                # If conversion fails, it's likely a categorical column, so leave it as is\n",
    "                pass\n",
    "        \n",
    "        # Convert datetime-like strings to actual datetime objects with a specific format if possible\n",
    "        if df_cleaned[column].dtype == 'object':\n",
    "            sample_value = df_cleaned[column].dropna().iloc[0]  # Take a sample value from the column\n",
    "            try:\n",
    "                # Check if the sample value looks like a date and if so, convert the entire column\n",
    "                if isinstance(pd.to_datetime(sample_value, format='%Y-%m-%d', errors='raise'), pd.Timestamp):\n",
    "                    df_cleaned[column] = pd.to_datetime(df_cleaned[column], format='%Y-%m-%d', errors='coerce')\n",
    "                elif isinstance(pd.to_datetime(sample_value, format='%d/%m/%Y', errors='raise'), pd.Timestamp):\n",
    "                    df_cleaned[column] = pd.to_datetime(df_cleaned[column], format='%d/%m/%Y', errors='coerce')\n",
    "                # Add other date formats as needed\n",
    "            except (ValueError, TypeError):\n",
    "                # If conversion fails, leave the column as is\n",
    "                pass\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Apply the function to the df_missing_values_handled DataFrame\n",
    "df_cleaned = ensure_consistency(df_missing_values_handled)\n",
    "\n",
    "# Optionally, display information about the cleaned DataFrame\n",
    "print(\"DataFrame info after ensuring consistency:\")\n",
    "print(df_cleaned.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset_characteristics(df):\n",
    "    print(\"### Dataset Characteristics ###\\n\")\n",
    "    \n",
    "    # Display the shape of the dataset\n",
    "    print(f\"Shape of the DataFrame: {df.shape}\")\n",
    "    \n",
    "    # Display the column data types\n",
    "    print(\"\\nData Types of Each Column:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Display summary statistics for numeric columns\n",
    "    print(\"\\nSummary Statistics for Numeric Columns:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Display the count of missing values per column\n",
    "    print(\"\\nCount of Missing Values per Column:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Display the count of unique values per column\n",
    "    print(\"\\nCount of Unique Values per Column:\")\n",
    "    print(df.nunique())\n",
    "    \n",
    "    # Display the first few rows of the cleaned dataset\n",
    "    print(\"\\nFirst Few Rows of the DataFrame:\")\n",
    "    print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
